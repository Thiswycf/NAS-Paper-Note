# 摘要
本文通过以可微分的方式制定任务来解决架构搜索的可扩展性挑战。与在离散且不可微分的搜索空间上应用进化或强化学习的传统方法不同，我们的方法基于架构表示的连续松弛，从而允许使用梯度下降高效地搜索架构。在 CIFAR-10、ImageNet、Penn Treebank 和 WikiText-2 上进行的大量实验表明，我们的算法在发现用于图像分类的高性能卷积架构和用于语言建模的递归架构方面表现出色，同时比最先进的不可微分技术快几个数量级。我们的实现已公开，以促进对高效架构搜索算法的进一步研究。


# 1. 绪论
发现最先进的神经网络架构需要人类专家付出大量努力。

最近，人们对开发算法解决方案以自动化架构设计的手动过程的兴趣日益浓厚。自动搜索的架构在图像分类（Zoph & Le，2017；Zoph 等人，2018；
Liu 等人，2018b；a；Real 等人，2018）和对象检测（Zoph 等人，2018）等任务中取得了极具竞争力的性能。

尽管现有的最佳架构搜索算法性能卓越，但计算要求很高。例如，获得 CIFAR-10 和 ImageNet 的最先进的架构需要 2000 GPU 天的强化学习 (RL)（Zoph 等人，2018）或 3150 GPU 天的进化（Real 等人，2018）。已经提出了几种加速方法，例如强加特定的搜索空间结构 (Liu et al., 2018b;a)、为每个单独的架构设置权重或性能预测 (Brock et al., 2018; Baker et al., 2018) 以及跨多个架构共享/继承权重 (Elsken et al., 2017; Pham et al., 2018b; Cai et al., 2018; Bender et al.,2018)，但可扩展性的根本挑战仍然存在。主流方法效率低下的内在原因，例如基于强化学习、进化算法、MCTS（Negrinho & Gordon，2017）、SMBO（Liu 等，2018a）或贝叶斯优化（Kandasamy 等，2018）的事实是，架构搜索被视为离散域上的黑盒优化问题，这导致需要进行大量的架构评估。在这项工作中，我们从不同的角度来处理这个问题，并提出了一种称为 DARTS（可微分架构搜索）的高效架构搜索方法。我们不是在一组离散的候选架构上进行搜索，而是将搜索空间放宽为连续的，这样就可以通过梯度下降针对其验证集性能对架构进行优化。与低效的黑盒搜索相比，基于梯度的优化的数据效率使 DARTS 能够使用数量级更少的计算资源实现与最先进技术相媲美的性能。它还优于另一种近期有效的架构搜索方法 ENAS（Pham 等人，2018b）。值得注意的是，DARTS **比许多现有方法更简单**，因为它不涉及控制器（Zoph & Le，2017；Baker 等人，2017；Zoph 等人，2018；Pham 等人，2018b；Zhong 等人，2018）、超网络（Brock 等人，2018）或性能预测器（Liu 等人，2018a），但它**足够通用**，可以处理卷积和循环架构。

在连续域内搜索架构的想法并不新鲜（Saxena & Verbeek，2016；Ahmed & Torresani，2017；Veniat & Denoyer，2017；Shin 等人，2018），但有几个主要区别。虽然先前的研究试图微调架构的特定方面，例如卷积网络中的过滤器形状或分支模式，但 DARTS 能够在丰富的搜索空间内学习具有复杂图形拓扑的高性能架构构建块。此外，DARTS 不限于任何特定的架构系列，并且适用于卷积和循环网络。在我们的实验（第 3 节）中，我们展示了 DARTS 能够设计一个卷积单元，使用 3.3M 参数在 CIFAR-10 图像分类中实现 2.76 ± 0.09% 的测试误差，这与使用三个数量级的计算资源获得的正则化进化（Real et al., 2018）的最新结果相媲美。相同的卷积单元在转移到 ImageNet（移动设置）时也实现了 26.7% 的 top-1 错误率，这与最佳 RL 方法（Zoph et al., 2018）相当。在语言建模任务中，DARTS 有效地发现了一个在 Penn Treebank (PTB) 上实现 55.7 测试困惑度的循环单元，其表现优于广泛调整的LSTM (Melis et al., 2018) 以及所有现有的基于 NAS (Zoph & Le, 2017) 和 ENAS (Pham et al., 2018b) 的自动搜索单元。

我们的贡献可以总结如下：
- 我们引入了一种基于双层优化的可微分网络架构搜索新算法，该算法适用于卷积和循环架构。
- 通过对图像分类和语言建模任务的大量实验，我们表明基于梯度的架构搜索在 CIFAR-10 上取得了极具竞争力的结果，并在 PTB 上超越了最先进的技术。这是一个非常有趣的结果，因为到目前为止，最好的架构搜索方法使用了不可微分的搜索技术，例如基于 RL（Zoph 等人，2018 年）或进化（Real 等人，2018 年；Liu 等人，2018b）。
- 我们实现了显着的效率提升（将架构发现的成本降低到几天的 GPU 时间），我们将其归因于使用基于梯度的优化而不是不可微分的搜索技术。
- 我们证明了 DARTS 在 CIFAR-10 和 PTB 上学习到的架构可以分别迁移到 ImageNet 和 WikiText-2。


# 2. 可微分架构搜索
我们在第 2.1 节中以一般形式描述了我们的搜索空间，其中架构（或其中的单元）的计算过程表示为有向无环图。然后，我们为我们的搜索空间引入了一个简单的连续松弛方案，从而为架构及其权重的联合优化提供了可微分学习目标（第 2.2 节）。最后，我们提出了一种近似技术，使算法在计算上可行且高效（第 2.3 节）。

## 2.1 搜索空间
按照 Zoph 等人 (2018 年)、Real 等人 (2018 年)、Liu 等人 (2018a;b) 的方法，我们寻找一个计算单元作为最终架构的构建块。学习到的单元可以堆叠以形成卷积网络，也可以递归连接以形成循环网络。

一个单元是一个有向无环图，由 $N$ 个节点的有序序列组成。每个节点 $x^{(i)}$ 都是一个潜在表示（例如卷积网络中的特征图），每个有向边 $(i,j)$ 都与某个转换 $x^{(i)}$ 的操作 $o^{(i,j)}$ 相关联。我们假设单元有两个输入节点和一个输出节点。对于**卷积**单元，**输入**节点定义为**前两层中的单元输出**（Zoph 等人，2018 年）。对于**循环**单元，这些定义为**当前步骤的输入和上一步携带的状态**。通过对所有中间节点应用归约运算（例如串联），可以获得单元的输出。

每个中间节点都是根据其所有前节点计算出来的：
$$
x^{(j)}= \sum_{i<j} {o^{(i,j)}(x^{(i)})}
$$

此外，还包含一个特殊的零运算，用于表示两个节点之间缺乏连接。因此，学习单元的任务就简化为学习单元边上的操作。

## 2.2 连续松弛和优化
让 $O$ 成为一组候选操作（如卷积、最大池化、归零），其中每个操作都代表要应用于 $x^{(i)}$ 的某个函数 $o(.)$。为了使搜索空间具有连续性，我们将特定操作的分类选择与所有可能操作的softmax值联系起来：
![image](/assets/image_rg8f81bz1.png)

其中，一对节点 (i,j) 的操作混合权重由维度为 |O| 的向量 α(i,j) 参数化。如图 1 所示，架构搜索任务简化为学习一组连续变量 α = {α(i,j)}。搜索结束时，用最有可能的操作替换每个混合操作 ̄o(i,j)，即 o(i,j) = argmaxo∈O α(i,j)o，就能得到一个离散架构。 在下文中，我们将 α 称为架构（编码）。

松弛之后，我们的目标是**联合学习架构α和所有混合运算中的权重 w**（例如卷积滤波器的权重）。与使用 **RL**（Zoph & Le，2017；Zoph 等人，2018；Pham 等人，2018b）或**进化**（Liu 等人，2018b；Real 等人，2018）进行架构搜索（其中**验证集性能被视为奖励或适应度**）类似，DARTS 的目标是优化验证损失，但使用的是梯度下降法。

用 $L_{train}$ 和 $L_{val}$ 分别表示训练损失和验证损失。这两种损失不仅取决于架构 α，还取决于网络中的权重 w。架构搜索的目标是找到能最小化验证损失 $L_{val}(w^*,α^*)$的 $α^*$，其中与架构相关的权重 $w^*$ 是通过最小化训练损失 $ w^*=argmin_w L_{train}(w,α^*) $得到的。

这意味着一个**双层优化问题**（Anandalingam 和 Friesz，1992 年；Colson 等人，2007 年），**α 为上层变量（架构），w 为下层变量（超参数）：**
![alt text](/assets/image.png)

嵌套公式也出现在**基于梯度的超参数优化**中（Maclaurin 等人，2015 年；Pedregosa，2016 年；Franceschi 等人，2018 年），从某种意义上说，架构 α 可被视为一种特殊类型的超参数，尽管其维度远远高于学习率等标量值超参数，而且更难优化。

![image](/assets/image_bchlvkump.png)

为每条边 (i,j) 创建一个以 $α^{(i,j)}$ 为参数的混合运算 $o^{(i,j)}$ 如果未收敛，则执行:
1. 通过下降 $∇_αL_{val}(w -ξ∇_wL_{train}(w,α),α)$ 更新结构 α（如果使用一阶近似，则 ξ = 0）
2. 通过下降 $∇_wL_{train}(w,α)$ 更新权重 w 根据学习到的α推导出最终架构。


## 2.3 近似架构梯度
由于昂贵的内部优化，准确估量架构梯度是非常高昂的，因此我们提出了一个简单的近似方案如下：

![alt text](/assets/image%20copy.png)

其中，w 表示算法保持的当前权重，ξ 是内部优化一步的学习率。这样做的目的是通过**仅使用一个训练步骤调整 w 来近似$w^{*}(α)$**，而无需通过训练直到收敛来完全解决内部优化（等式 4）。 相关技术已被用于模型转移的元学习（Finn 等人，2017 年）、基于梯度的超参数调整（Luketina 等人，2016 年）和未卷生成对抗网络（Metz 等人，2017 年）。注意到如果$w$已经是最优值，则下面的等式将会缩减到$∇_αL_{val}(w,α)$并因此$∇_αL_{train}(w,α) = 0$

迭代过程如图 1 所示。1.虽然我们目前还不清楚我们的优化算法的收敛性保证，但在实践中，只要选择合适的ξ，它就能达到一个固定点。我们还注意到，当权重优化启用动量时，等式 6 中的单步未滚动学习目标也会相应修改，我们的所有分析仍然适用。

将链式法则应用于近似结构梯度（**公式 6 - 上图下式**），得出

![alt text](/assets/image%20copy%202.png)

推导如下：

$$
\begin{align}
\nabla_{\alpha} w' &= \frac {\partial (w-ξ∇_wL_{train}(w,α))} {\partial \alpha}
\\
&= -ξ \cdot \frac {\partial (∇_wL_{train}(w,α))} {\partial \alpha}
\\
&= -ξ \cdot \frac {\partial^2 L_{train}(w,α)} {\partial \alpha \partial w}
\\
&= -ξ \cdot \nabla^2_{\alpha, w} L_{\text{train}}(w,α)
\\
\end{align}
$$

$$
\begin{align}
\text {equation 6} &=
∇_α(L_{val}(w-ξ∇_wL_{\text{train}}(w,α),α)) \\ 
&= ∇_α(L_{val}(w^′,α))
\\
&= \nabla_{\alpha} w' \cdot ∇_{w'}L_{val}(w^′,α) + ∇_αL_{val}(w^′,α)
\\
&= -ξ \nabla^2_{\alpha, w} L_{\text{train}}(w,α)∇_{w'}L_{val}(w^′,α) + ∇_αL_{val}(w^′,α)
\\
\end{align}
$$

其中，$w^′ = w-ξ∇_wL_{\text{train}}(w,α)$ 表示一步前向模型的权重。上面的表达式在第二项中包含了一个昂贵的矩阵向量积。幸运的是，使用有限差分近似法可以大大降低复杂性。
<mark>假设 $\epsilon$ 是一个小标量，且 $w^± = w ± \epsilon ∇_{w^′}L_{\text{val}}(w^′,α)$ 。那么：
$$
\nabla_{\alpha,w}^{2}\mathcal{L}_{train}(w,\alpha)\nabla_{w'}\mathcal{L}_{val}(w',\alpha)\approx\frac{\nabla_{\alpha}\mathcal{L}_{train}(w^{+},\alpha)-\nabla_{\alpha}\mathcal{L}_{train}(w^{-},\alpha)}{2\epsilon} \quad ??
$$
$$
\nabla_{\alpha,w}^{2}\mathcal{L}_{train}(w,\alpha)\nabla_{w'}\mathcal{L}_{val}(w',\alpha)\approx\frac{\nabla_{\alpha}\mathcal{L}_{train}(w^{+},\alpha)-\nabla_{\alpha}\mathcal{L}_{train}(w^{-},\alpha)}{2\epsilon}\mathbf{\nabla_{w'}\mathcal{L}_{val}(w',\alpha)}
$$

计算有限差分只需对权重进行两次前向运算，对 $α$ 进行两次后向运算，并且时间复杂度从 $O(|\alpha||w|)$ 降到了 $O(|\alpha| + |w|)$

一阶近似 当 $ξ = 0$ 时，等式 7 中的二阶导数将消失。 在这种情况下，架构梯度由 $∇_α\mathcal{L}_{val}(w,α)$ 给出，对应于通过假设当前 $w$ 与 $w^∗(α)$ 相同来优化验证损失的简单启发式。表 1 和表 2 中的实验结果表明，这种方法虽然提高了计算速度，但实际性能却较差。在下文中，我们将 $ξ = 0$ 的情况称为一阶近似，将 $ξ > 0$ 的梯度公式称为二阶近似。


## 2.4 导出离散架构
为了形成离散结构中的每个节点，我们保留了从之前所有节点收集的所有非零候选操作中（来自不同节点的）最强的前 k 个操作。操作的强度定义为
$$
\frac{\exp(\alpha_o^{(i,j)})}{\sum_{o'\in\mathcal{O}}\exp(\alpha_{o'}^{(i,j)})}
$$
为了使我们派生的架构与现有作品中的架构相当，我们对卷积单元使用 $k = 2$（Zoph 等，2018；Liu 等，2018a；Real 等，2018），对循环单元使用 $k = 1$（Pham 等，2018b）。


# 3. 试验和结果
我们在 CIFAR-10 和 PTB 上进行的实验包括两个阶段：架构搜索（第 3.1 节）和架构评估（第 3.2 节）。在第一阶段，我们使用 DARTS 搜索单元架构，并根据其验证性能确定最佳单元。在第二阶段，我们使用这些单元构建更大的架构，从头开始训练，并报告它们在测试集上的性能。我们还分别在 ImageNet 和 WikiText-2 (WT2) 上评估了在 CIFAR-10 和 PTB 上学习到的最佳单元，从而研究了它们的可移植性。


## 3.1 架构搜索
我们在 $O$ 中包含了以下操作：3 × 3 和 5 × 5 可分离卷积、3 × 3 和 5 × 5 扩张可分离卷积、3 × 3 最大池化、3 × 3 平均池化、同一性和零操作。所有操作的步长均为 1（如适用），卷积后的特征图经过填充以保持其空间分辨率。我们使用 ReLU-Conv-BN 顺序进行卷积操作，每个可分离卷积总是应用两次（Zoph 等人，2018 年；Real 等人，2018 年；Liu 等人，2018a）。

我们的卷积单元由 N = 7 个节点组成，其中输出节点被定义为所有中间节点（不包括输入节点）的深度连接。其余设置遵循 Zoph 等人（2018）；Liu 等人（2018a）；Real 等人（2018）的方法，即通过将多个单元堆叠在一起形成网络。单元 k 的第一和第二节点分别等于单元 k-2 和单元 k-1 的输出，并根据需要插入 1 × 1 卷积。位于网络总深度 1/3 和 2/3 处的单元格为降维单元（reduction cells）格，其中所有与输入节点相邻的操作步长均为 2。因此，架构编码为 ($α_{normal}$,$α_{reduce}$)，其中 $α_{normal}$ 由所有正常单元共享，$α_{reduce}$ 由所有降维单元（reduction cells）共享。

本节的详细实验设置见第 A.1.1 节。A.1.1.


### 3.1.2 在PENN树状库中搜索递归单元格
我们的可用运算集包括线性变换，然后是 tanh、relu、sigmoid 激活中的一种，以及身份映射和零操作。这些候选运算的选择遵循 Zoph & Le（2017）；Pham 等人（2018b）。

我们的循环单元由 N = 12 个节点组成。第一个中间节点是通过线性变换两个输入节点、将结果相加然后通过 tanh 激活函数获得的，就像在 ENAS 单元中所做的那样（Pham 等人，2018b）。单元的其余部分是学习而来的。其他设置类似于 ENAS，其中每个操作都通过高速旁路增强（Zilly 等人，2016），单元输出定义为所有中间节点的平均值。与 ENAS 一样，我们在每个节点中启用批量归一化以防止在架构搜索期间出现梯度爆炸，并在架构评估期间禁用它。我们的循环网络仅由单个单元组成，即我们不假设循环架构中存在任何重复模式。

![Alt](/assets/image%20copy%203.png)

> 图 3：DARTS 对 CIFAR-10 卷积单元和 Penn Treebank 循环单元的搜索进度。我们一直在跟踪最新的架构。我们使用训练集对每个架构快照从头开始重新训练（在 CIFAR-10 上训练 100 个历时，在Penn Tree上训练 300 个历时），然后在验证集上进行评估。对于每项任务，我们使用不同的随机种子重复实验 4 次，并报告架构随时间变化的中位数和最佳（每次运行）验证性能。作为参考，我们还报告了使用 RL 或进化发现的现有最佳单元的结果（在相同的评估设置下；参数数量相当），包括 NASNet-A（Zoph 等人，2018 年）（2000 GPU 天）、AmoebaNet-A（3150 GPU 天）（Real 等人，2018 年）和 ENAS（0.5 GPU 天）（Pham 等人，2018b）。

![Alt](/assets/image%20copy%204.png)


## 3.2 架构评估
为了确定最终评估的架构，我们使用不同的随机种子运行 DARTS 四次，并根据从头开始训练一小段时间（在 CIFAR-10 上训练 100 个周期，在 PTB 上训练 300 个周期）获得的验证性能挑选出最佳单元。这**对于循环单元尤其重要**，因为优化结果可能**对初始化敏感**（图 3）。

为了评估所选架构，我们随机初始化其权重（搜索过程中学习到的权重将被丢弃），从头开始训练，并报告其在测试集上的表现。我们注意到，测试集从未用于架构搜索或架构选择。

在 CIFAR-10 和 PTB 上进行架构评估的详细实验设置见第 A.2.1 节和第 A.2.2.2 节。A.2.1 节和 A.2.2 节。A.2.2 节。除了 CIFAR-10 和 PTB，我们还分别在 ImageNet（移动环境）和 WikiText-2 上对最佳卷积单元（在 CIFAR-10 上搜索）和循环单元（在 PTB 上搜索）进行了评估，从而进一步研究了它们的迁移能力。有关迁移学习实验的更多详情，请参阅第 A.2.3 节和第 A.2.4 节。A.2.3 节和 A.2.4 节。A.2.4.

> 表 1：与 CIFAR-10 上最先进的图像分类器的比较（错误率越低越好）。 请注意，DARTS 的搜索成本不包括选择成本（1 GPU 天）或从头开始训练所选架构的最终评估成本（1.5 GPU 天）。
![Alt](/assets/image%20copy%205.png)
\* 使用作者公开发布的代码重复 ENAS 8 次获得。用于最终评估的单元根据与 DARTS 相同的选择协议进行选择。
† 通过使用我们的设置训练相应的架构获得。
‡ 根据 100 次训练历时后的验证误差，24 个样本中的最佳架构。

> 表 2：与 PTB 上最先进语言模型的比较（困惑度越低越好）。请注意，DARTS 的搜索成本不包括选择成本（1 GPU 天）或从头开始训练所选架构的最终评估成本（3 GPU 天）。
![Alt](/assets/image%20copy%206.png)
\* 使用作者公开发布的代码（Pham 等人，2018a）获得。
† 使用我们的设置训练相应的架构获得。
‡ 根据 300 次训练历时后的验证困惑度，在 8 个样本中获得最佳架构。

> 表 3：在移动环境中与 ImageNet 上最先进的图像分类器比较
![Alt](/assets/image%20copy%207.png)


## 3.3 结果分析
表 1 列出了卷积架构的 CIFAR-10 结果。值得注意的是，DARTS 取得了与最先进技术（Zoph 等人，2018 年；Real 等人，2018 年）相当的结果，同时使用的计算资源少了三个数量级（即 1.5 或 4 GPU 天 vs NASNet 的 2000 GPU 天和 AmoebaNet 的 3150 GPU 天）。此外，在搜索时间稍长的情况下，DARTS 的表现优于 ENAS（Pham 等人，2018b），其发现的单元错误率相当，但参数更少。搜索时间较长的原因是，我们在选择单元时**重复了四次搜索过程**。不过，这种做法**对于卷积单元来说并不那么重要**，因为所发现的架构的性能并**不严重依赖于初始化**（图 3）。

**替代优化策略** （第一个实验），为了更好地理解双层优化的必要性，我们研究了一种简单的搜索策略，即使用**坐标下降法**在训练集和验证集的结合处联合优化 α 和 w。结果得出的最佳卷积单元（4 次运行中）在使用 310 万个参数的情况下，测试误差为 4.16 ±0.16%，**比随机搜索更差**。
<details>
  <summary>坐标下降法（CD）和随机梯度下降法（SGD）解释（点击展开）</summary>
  <br>坐标下降法（Coordinate Descent，CD）和随机梯度下降法（Stochastic Gradient Descent，SGD）都是用于优化多变量函数的算法，它们在机器学习中尤其常用。以下是CD和SGD的主要区别：
  <br>1. 优化策略的不同：
  <br>- 坐标下降法（CD）：在每一步迭代中，CD选择一个坐标（即一个参数），然后固定其他所有参数，仅在这个坐标方向上优化目标函数。这意味着每次迭代只更新一个参数。
  <br>- 随机梯度下降法（SGD）：SGD在每一步迭代中更新所有参数。它通过计算随机选择的样本或小批量样本的梯度来更新参数，而不是沿着一个坐标方向。
  <br>2. 梯度计算的不同：
  <br>- CD：在每一步中，CD计算的是在固定其他参数的情况下，沿着选定坐标方向的目标函数梯度。
  <br>- SGD：SGD计算的是整个目标函数的梯度，但只基于随机选择的样本或小批量样本。
  <br>
  <br>3. 收敛速度和稳定性：
  <br>- CD：CD可能在某些情况下更快地收敛，尤其是当目标函数沿着某些坐标方向更容易优化时。然而，如果坐标选择不当，CD可能会收敛得非常慢。
  <br>- SGD：SGD的收敛路径通常更加波动，但由于它考虑了多个参数的相互作用，它可能更快地找到最小值。SGD也更容易逃离局部最小值。
  <br>4. 对大数据集的适应性：
  <br>- CD：对于大数据集，CD可能不是最优选择，因为它需要多次迭代，每次迭代只更新一个参数。
  <br>- SGD：SGD非常适合大规模数据集，因为它每次迭代只处理部分数据，从而降低了计算成本。
  
</details><br>

在第二个实验中，我们使用 SGD 同时优化了 α 和 w（未做任何改动），同样是在所有可用数据（训练集 + 验证集）上进行优化。在使用 3.0M 个参数的情况下，得出的最佳单元测试误差为 3.56 ±0.10% 。我们假设这些启发式方法会导致 α（类似于超参数）过度拟合训练数据，从而导致泛化效果不佳。请注意，在 DARTS 中，α 并未在训练集上直接优化（在测试集中）。

表 2 列出了递归架构在 PTB 上的测试结果，其中由 DARTS 发现的单元的测试困惑度为 55.7。这与通过混合softmax增强的最先进模型不相上下（Yang et al.需要注意的是，我们自动搜索的单元优于广泛调整的 LSTM（Melis 等人，2018 年），这表明除了超参数搜索之外，架构搜索也很重要。在效率方面，总体成本（共运行 4 次）在 1 GPU 天之内，与 ENAS 不相上下，明显快于 NAS（Zoph & Le，2017 年）。

值得注意的是，随机搜索对卷积模型和递归模型都有竞争力，这反映了搜索空间设计的重要性。然而，在搜索成本相当或更低的情况下，DARTS 在这两种情况下都能显著提高随机搜索的性能（CIFAR-10 为 2.76 ± 0.09 vs 3.29 ± 0.15；PTB 为 55.7 vs 59.4）。

表 3 中的结果表明，在 CIFAR-10 上学习到的单元确实可以移植到 ImageNet 上。值得注意的是，DARTS 的性能与最先进的 RL 方法（Zoph 等人，2018 年）相比具有竞争力，而使用的计算资源却少了三个数量级。

表 4 显示，DARTS 识别出的单元在 WT2 中的转移性优于 ENAS，但总体结果不如表 2 中 PTB 的结果。PTB 和 WT2 之间的可转移性较弱（与 CIFAR-10 和 ImageNet 之间的可转移性相比），原因可能是架构搜索的源数据集（PTB）相对较小。在感兴趣的任务上直接优化架构有可能避免可移植性问题。

> 表 4：在 WT2 上与最先进语言模型的比较
![Alt](/assets/image%20copy%208.png)
† 通过使用我们的设置训练相应的架构获得。

<br>

# 4. 总结
我们介绍了 DARTS，这是一种适用于卷积网络和递归网络的简单而高效的架构搜索算法。通过在连续空间中搜索，DARTS 能够在图像分类和语言建模任务中媲美或超越最先进的无差别架构搜索方法，效率显著提高了几个数量级。 进一步改进 DARTS 有很多有趣的方向。例如，目前的方法可能会受到连续架构编码和衍生离散架构之间差异的影响。例如，可以通过退火softmax温度（采用合适的时间表）来执行单次选择，从而缓解这一问题。此外，基于**搜索过程**中学习到的**共享参数**，研究**性能感知架构**推导方案也很有意义。
